{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f34167f",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c4a06",
   "metadata": {},
   "source": [
    "We use lower case letters, such as a,b,c to denote the vectors/scalers. It should\n",
    "be clear from the contexts a letter refers to vector or scaler. For matrices, we\n",
    "use upper-case letters, such as A,B,C. In addition, we sometimes make use of\n",
    "“tensors”, which are nothing but high-dimensional arrays. More specifically, we\n",
    "call vectors 1-d arrays, matrices 2-d arrays, and tensors 3-d or 4-d arrays. We\n",
    "also use upper-case letters to represent tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d25a6",
   "metadata": {},
   "source": [
    "我们用小写字母，比如a,b,c表示向量或标量，通过上下文可以清楚的了解到一个字母是向量还是标量。我们用大写字母表示矩阵，如A,B,C。另外，我们有时使用“张量”这一概念表示高维矩阵。更准确的说，我们称一维数组为矢量，二维数组为矩阵，3维或4维数组为矢量。我们同样使用大写字母表示张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56617e",
   "metadata": {},
   "source": [
    "Given a vector a, we call it N-dimensional, if it contains exactly N elements,\n",
    "and we use $a_i$ to denote its i’th element, 1 ≤ i ≤ N. Similarly, for a matrix A,\n",
    "we call it M ×N dimensional, if it contains M rows and N columns, and we\n",
    "use Aij or Ai,jto denote its (i,j)’th element, 1 ≤ i ≤ M,1 ≤ j ≤ N. We use\n",
    "similar notations for tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d846c17",
   "metadata": {},
   "source": [
    "一个矢量，如果包含N个元素，我们称为N维矢量。我们使用$a_i$表示第i个元素。同样对于一个$M \\times N$矩阵A，我们使用$A_{ij}$表示其第$i,j$个元素。我们使用同样的方式标记张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bcb21",
   "metadata": {},
   "source": [
    "We follow the conventions in literature when it comes to the sum/difference\n",
    "of vectors/matrices. Similarly, we follow the conventions in the literature in\n",
    "representing the product between scalers/vectors/matrices. We omit the details\n",
    "here, and we refer the readers to Schott (2016).\n",
    "In addition, we also need the following vector/matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6bcdd",
   "metadata": {},
   "source": [
    "我们在矩阵或向量的差、和、乘积方面遵守写作惯例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6161767",
   "metadata": {},
   "source": [
    "## Vectorization of matrices，矩阵的拉直"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51de4e",
   "metadata": {},
   "source": [
    "Let A be an M ×N matrix. We use vec(A) to denote the vector obtained by\n",
    "stacking the columns of A. More specifically, let $a_i$ denote the i'th column of\n",
    "A, then $vec(A)=[a_1^t,a_2^t,...,a_n^t]^t$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0e2a7",
   "metadata": {},
   "source": [
    "A表示一个$M \\times N$矩阵，我们使用$vec(A)$表示将A按列拉平，即$a_i$表示A的第i列，$vec(A)=[a_1^t,a_2^t,...,a_n^t]^t$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754fde2",
   "metadata": {},
   "source": [
    "## Inner product 内积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e8da7",
   "metadata": {},
   "source": [
    "Let a,b be N-dimensional vector. Their inner product $a \\cdot b= a^tb=\\sum_{i=1}^{N} a_ib_i$\n",
    "Similarly, let A,B be M ×N-dimensional matrices, then their inner product $A\\cdot B= \\sum_{j=1}^{M}\\sum_{i=1}^{N} A_{ij}B_{ij}$，**要求A与B的维度相同，计算结果是标量**。  \n",
    "We have the following identity:  \n",
    "$A\\cdot(BC)=(B^tA)\\cdot C=(AC^t)\\cdot B$  \n",
    "内积：只要形式上能相乘，结果就是相等的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e2e63",
   "metadata": {},
   "source": [
    "## Hadamard product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f4285",
   "metadata": {},
   "source": [
    "Let A,B be M ×N-dimensional matrices. Their Hadamard product, $A\\bigodot B$ is\n",
    "defined as an M ×N-dimensional matrix which satisfies,  \n",
    "$(A\\bigodot B)_{ij}=A_{ij}B_{ij}$  \n",
    "We have the following identity  \n",
    "$A\\cdot(B\\bigodot C)=(A\\bigodot B)\\cdot C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1cda6",
   "metadata": {},
   "source": [
    "Hadamard product要求两个矩阵维度相同，乘积的结果为两个矩阵对位相乘。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828df82",
   "metadata": {},
   "source": [
    "# Vector/matrix calculus 矢量与矩阵的微积分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d45d51",
   "metadata": {},
   "source": [
    "In this section, we derive the identities of vector/matrix calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b283a3",
   "metadata": {},
   "source": [
    "## Directives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f4bc5",
   "metadata": {},
   "source": [
    "Let Q be an $R^M \\to  R$ function, and we use  $Q'(x)　or　\\frac{\\partial Q}{\\partial x} $　to denote the following　M-dimensional vector,  \n",
    "$Q'(x)_i=\\frac{\\partial Q}{\\partial x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa24c54",
   "metadata": {},
   "source": [
    "Q为一个将M维向量映射为标量的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa7fc9",
   "metadata": {},
   "source": [
    "Similarly, let Q be a $R^{M \\times N} \\to  R$ functional, we use $Q'(X)　or　\\frac{\\partial Q}{\\partial X} $ to denote the\n",
    "following M ×N-dimensional matrix,  \n",
    "$Q'(X)_{ij}=\\frac{\\partial Q}{\\partial X_{ij}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bc109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T16:03:31.774004Z",
     "start_time": "2021-11-01T16:03:31.657175Z"
    }
   },
   "source": [
    "Q为一个将M,N维矩阵映射为标量的函数，注意此时的X为upper case letter，表示矩阵。导数为$Q$对每个$x_i$求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66745c1",
   "metadata": {},
   "source": [
    "Now let Q be a function from $R^{M} \\to  R^N $, s.t.  \n",
    "$Q(x)=\\begin{bmatrix}\n",
    "Q_1(x)\\\\  \n",
    "Q_2(x)\\\\  \n",
    "\\cdot\\\\  \n",
    "\\cdot\\\\  \n",
    "\\cdot\\\\  \n",
    "Q_N(x)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Then its Jacobian $\\nabla Q(x)$ is defined as an N × M-dimensional matrix that\n",
    "satisfies,  \n",
    "$\\nabla Q(x)_{ij}=\\frac{Q_i(x)}{\\partial x_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcede7f",
   "metadata": {},
   "source": [
    "1.Q(x)的形式是有限的，是一个N维向量，每个元素是一个函数。  \n",
    "2.$\\nabla Q(x)_{ij}=\\frac{Q_i(x)}{\\partial x_j}$，即以$Q_i(x)$对每个$x_j$求导为矩阵的行，结果为一个$N \\times M$的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5dd883",
   "metadata": {},
   "source": [
    "插曲：  \n",
    "Usually, the acronym $s.t.$ means such that. In the context of optimization, it means subject to. Also note that such that does not have the same meaning as so that.  \n",
    "Such that, describes how something should be done.  \n",
    "So that, describes why something should be done.   \n",
    "For clarity, it's usually best to avoid $s.t.$ and simply write such that.  \n",
    "s.t.是subject to （such that）的缩写，受约束的意思。按中文习惯可以翻译成：使得...满足..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea92fa",
   "metadata": {},
   "source": [
    "For functions that maps matrices/tensors to matrices/tensors, we define the derivatives as the corresponding vectorized version. To illustrate, let Q be a function that maps $R^{M1 \\times N1}$ to $R^{M2 \\times N2}$, then its derivatives are defined as <font size=\"5\"> $\\frac{\\partial vec(Q)}{\\partial vec^t(X)}$</font>  即“先把$Q_i$拉平，再把$X$拉平，之后挨个求导。个人理解：$Q$为$R^{M2 \\times N2}$,$X$为$R^{M1 \\times N1}$维，计算结果为$(M2\\times N2)  \\times (M1 \\times N1)$维的矩阵”  \n",
    "We have the following formula:  \n",
    "<font size=\"5\">$\\frac{\\partial vec(Q(F(X)))}{\\partial vec^t(X)}=\\frac{\\partial vec(Q(F(X))}{\\partial vec^t (F(X))}\\frac{\\partial vec(F(X))}{\\partial vec^t (X)}$</font>  \n",
    "  \n",
    "即“按照链式求导法则，一层一层的拉平，求导，这个具体的维度，就很麻烦了”  \n",
    "个人理解：比如：$Q:M1\\times N1,F:M2 \\times N2,X:M3 \\times N3$，则$\\frac{\\partial vec(Q(F(X))}{\\partial vec^t (F(X))}$为$(M1\\times N1)\\times (M2\\times N2)$,$\\frac{\\partial vec(F(X))}{\\partial vec^t (X)}$为$(M2\\times N2 \\times (M3\\times N3)$，最终结果为$(M1\\times N1)\\times (M3\\times N3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d40d3f",
   "metadata": {},
   "source": [
    "还有：\n",
    "<font size=\"5\">$\\frac{\\partial Mx}{\\partial x^t}=M$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1662ef",
   "metadata": {},
   "source": [
    "## Differentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24437c6c",
   "metadata": {},
   "source": [
    "Let Q be a function that maps $R^M \\mapsto R^N$, and assume its derivative exists. It can be proved that there exists a matrix Ax s.t.,  \n",
    "$Q(x+dx)=Q(x)+A_xdx+o(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d7ff9",
   "metadata": {},
   "source": [
    "Q是一个$R^M \\mapsto R^N$的映射，假设其导数存在，可证明存在一个矩阵$A_x$，使得：  \n",
    "$Q(x+dx)=Q(x)+A_xdx+o(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87075bfd",
   "metadata": {},
   "source": [
    "In the display above, we use o(dx) to denote an N-dimensional vector that satisfies,  \n",
    "<font size=\"5\">$\\lim_{dx \\to 0}\\frac{o(dx)}{ \\lVert dx  \\rVert}=0$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b3fc4",
   "metadata": {},
   "source": [
    "We denote the differential of $Q,dQ(x;dx): R^M \\mapsto R^N$, s.t. $dQ(x;dx)=A_xdx$  \n",
    "We also write $dQx.dx$ in place of $dQ(x;dx)$.  \n",
    "$Q$的导数为$dQ(x;dx),dQ(x;dx)=A_xdx$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f60d5",
   "metadata": {},
   "source": [
    "Similarly, let Q be a function that maps $R^{M1 \\times N1}$ to $R^{M2 \\times N2}$, and assume it is element-wise differentiable. Then there exists matrix $A_X$ s.t.  \n",
    "$vec(Q(X+dX))=vec(Q(X)+A_xvec(dX)+o(dX)$  \n",
    "同样，$Q$一个 $R^{M1 \\times N1}$ to $R^{M2 \\times N2}$映射，假设其所有元素可微，则存在一个矩阵$A_X$，使得：  \n",
    "$vec(Q(X+dX))=vec(Q(X)+A_Xvec(dX)+o(dX)$  \n",
    "要搞矩阵看来都要拉平了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86868767",
   "metadata": {},
   "source": [
    "We define the differential of $Q,dQ(X;dX)$ to be an $M2 \\times N2$ matrix s.t.  \n",
    "$vec(dQ(X;dX))=A_Xvec(dX)$  \n",
    "We also use $dQ_X.dX$ to denote $dQ(X;dX)$.  \n",
    "把$dQ(X;dX)$记为$Q$的导数，$vecdQ(X;dX)=A_Xvec(dX)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1b77a",
   "metadata": {},
   "source": [
    "The derivatives and differentials are related through the following implications:  \n",
    "$dQ_X.dX=M \\cdot dX$   \n",
    "is equivalent to  \n",
    "$\\frac{\\partial Q(X)}{\\partial X}=M$\n",
    "\n",
    "We have the following chain rules for differentials,  \n",
    "$dQ_X.dX=dG_F.dF_X.dX$  \n",
    "\n",
    "In view of this we write can omit $dX$ and the subscript to write  \n",
    "$dQ=dG \\cdot dF$  \n",
    "\n",
    "Finally, we have the following identities,  \n",
    "\\begin{align}\n",
    "d(\\alpha X+Y)&=\\alpha dX+dY\\\\\n",
    "d(XY)&=(dX)Y+XdY\\\\\n",
    "d(X\\cdot Y)&=dX \\cdot Y +X\\cdot dY\\\\\n",
    "d(X\\bigodot Y)&=dX \\bigodot Y +X \\bigodot dY\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ccbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
